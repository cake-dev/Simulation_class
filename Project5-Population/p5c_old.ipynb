{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "cae80105",
      "metadata": {},
      "source": [
       "# Project 5c-SIR models\n",
       "\n",
       "A classic epidemiological model is called the SIR model, which stands for Susceptible-Infected-Recovered (or more broadly Removed, if the disease is potentially fatal).  As the name implies, this is a population model with three populations.  The equations are pretty simple (even simpler than zombies):\n",
       "$$\n",
       "\\frac{\\partial S}{\\partial t} = -\\beta S I\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial I}{\\partial t} = \\beta S I - \\gamma I\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial R}{\\partial t} = \\gamma I,\n",
       "$$\n",
       "yet they have rather successfully been used to model a broad array of epidemics, including, in a practical sense, COVID-19.  Implement the above equations and explore their behavior for a few combinations of parameters and initial conditions.  What are the stable states?  Does this model exhibit periodicity?"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "1816c741",
      "metadata": {},
      "outputs": [],
      "source": [
       "import numpy as np\n",
       "import ode_methods as om\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "class SIR:\n",
       "    \n",
       "    def __init__(self,beta=10,gamma=0.1):\n",
       "        \n",
       "        self.beta = beta    # Infection rate\n",
       "        self.gamma = gamma  # Recovery/death rate\n",
       "        self.n_dof = 3      \n",
       "        \n",
       "    def rhs(self,t,u):\n",
       "        # 0 - S\n",
       "        # 1 - I\n",
       "        # 2 - R\n",
       "        \n",
       "        # the right hand side of the ode (or $\\mathcal{F}(t,u)$)\n",
       "        dudt = np.array([-self.beta*u[0]*u[1],\n",
       "                         self.beta*u[0]*u[1] - self.gamma*u[1],\n",
       "                         self.gamma*u[1]])  \n",
       "\n",
       "        return dudt\n",
       "    \n",
       "method = om.Midpoint()\n",
       "\n",
       "u0 = np.array([1,0.1,0])\n",
       "\n",
       "s = SIR()\n",
       "integrator = om.Integrator(s,method)\n",
       "t,u = integrator.integrate([0,100],0.01,u0)\n",
       "\n",
       "plt.plot(t,u[:,0],label='Susceptible')\n",
       "plt.plot(t,u[:,1],label='Infected')\n",
       "plt.plot(t,u[:,2],label='Recovered')\n",
       "plt.legend()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4bf37c9c",
      "metadata": {},
      "source": [
       "### Comparison to observations\n",
       "\n",
       "While it is useful to examine the qualitative evolution of these systems, they are not necessarily useful for modelling real epidemics because it is not clear *a priori* what the values of the parameters ought to be.  In order to do that, they need to be tuned so as to reproduce a set of real observations.  I have provided for you code to read in and process data from the first year of the pandemic in Montana."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "74eab725",
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "data = pd.read_csv('ef7e2f14.csv')\n",
       "\n",
       "N = 1.05e5 # Initial susceptible population, here taken to be approximately \n",
       "          # 1/10 of the actual population of the state\n",
       "          # or around the same amount of people that end up \n",
       "          # getting infected\n",
       "removed = data['recovered'] + data['deaths']\n",
       "infected = data['confirmed'] - removed\n",
       "susceptible = N - infected - removed\n",
       "\n",
       "removed = removed[removed.notnull()].to_numpy()[100::7]\n",
       "infected = infected[infected.notnull()].to_numpy()[100::7]\n",
       "susceptible = susceptible[susceptible.notnull()].to_numpy()[100::7]\n",
       "times = np.linspace(0,len(susceptible)-1,len(susceptible))\n",
       "\n",
       "\n",
       "plt.plot(times,susceptible,'b.',label='susceptible')\n",
       "plt.plot(times,infected,'r.',label='infected')\n",
       "plt.plot(times,removed,'g.',label='removed')\n",
       "plt.legend()\n",
       "plt.xlabel('weeks')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "a3f38ef5",
      "metadata": {},
      "source": [
       "**\"fit\" the data, which is to say: come up with values for $\\gamma$, $\\beta$, and $I_0$ such that predictions from the model agree reasonably well with the observations.**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fdc01e",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "8235051d",
      "metadata": {},
      "source": [
       "## Markov Chain Monte Carlo sampling (a more than brief aside)\n",
       "Tuning models by hand is a tricky task, especially as the number of parameters gets large.  It's also quite subjective, and might not capture all of the possible configurations of the model that could fit the data equivalently well.  As such, we'd like to come up with an automated procedure for fitting models to data.  \n",
       "\n",
       "This is of course, a rather vast subject.  Indeed, fitting (very flexible) models (with lots of parameters) to data is most of the content of a machine learning class.  In that context, the common practice is to define a cost function (commonly mean squared error) and minimize it using an algorithm called gradient descent.  Such an approach is possible here as well, but requires the efficient differentiation of the ordinary differential equation that we're solving itself using something called [the adjoint method](https://en.wikipedia.org/wiki/Adjoint_state_method).  This is a very interesting topic, but is a bit outside of the scope of our course.\n",
       "\n",
       "Another downside to gradient descent is that it provides only a single set of optimal model parameters.  However, for real scientific applications, it's typically very useful for us to know something about the *distribution* of possible values that the parameters could take.  For that reason, we'll try to characterize our parameters as probability distributions.  To characterize them, we'll use a tool called the Metropolis-Hastings algorithm. \n",
       "\n",
       "### A small example from ballistics\n",
       "Let's go back to a very simple ballistics problem, namely two dimensional motion without drag.  Recall that the equations were\n",
       "$$\n",
       "\\frac{\\partial \\mathbf{x}}{\\partial t} = \\mathbf{v}\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial \\mathbf{v}}{\\partial t} = \\mathbf{g}\n",
       "$$\n",
       "where $\\mathbf{x}$ is the position vector, $\\mathbf{v}$ the velocity vector, $\\mathbf{g}=[0,-9.81]^T$ the gravity vector.  The analytical solution to this is \n",
       "$$\n",
       "\\mathbf{x}(t) = \\mathbf{x}_0 + \\mathbf{v}_0 t + \\frac{1}{2}\\mathbf{g} t^2\n",
       "$$\n",
       "Let's say I *do* know the location from which the projectile was launched, but that I *don't* know its initial velocity.  This latter quantity I would like to infer.  I do have a noisy measurement of its position at some point in time.  As an example, lets say at $t = 1.0$s, I measure its position to be $\\mathbf{x}(1s) = [10,3]$m.  What can I say about its initial position and velocity?  \n",
       "\n",
       "Bayes' theorem gives us an answer:\n",
       "$$\n",
       "P(\\mathbf{v}_0|\\hat{\\mathbf{x}}_1) = \\frac{P(\\hat{\\mathbf{x}}_1|\\mathbf{v}_0) P(\\mathbf{v_0})}{P(\\hat{\\mathbf{x}}_1)},\n",
       "$$\n",
       "where I've used a hat to indicate an observed quantity.  \n",
       "\n",
       "The term on the left hand side is what we want: the probability distribution of the initial position and velocity given the observed position at some time.  The right hand side includes things that we can make assumptions about.  $P(\\mathbf{v}_0)$ is a *prior* distribution, which encodes the distribution of possible initial velocities before looking at any data.  Let's assume that this is proportional to 1, which is equivalent to saying that I don't know anything about the velocity to start with.  \n",
       "\n",
       "The other term $P(\\mathbf{x}_1|\\mathbf{v}_0)$ is called a *likelihood*, and it asks the question \"what is the probability of making the observation ($\\hat{\\mathbf{x}}_1$) for some given value of $\\mathbf{v}_0$.  Answering this question has two parts here: first, I need to map from $\\mathbf{v}_0$ to a hypothesized position at $t=1$s.  This is is easy to do with our model.  Define\n",
       "$$\n",
       "\\mathbf{x}_1(\\mathbf{v}_0) \\equiv \\mathbf{x}_0 + \\mathbf{v}_0 (1) + \\frac{1}{2} \\mathbf{g} (1)^2.\n",
       "$$\n",
       "Second, we need to make an assumption about uncertainty in our measurement.  A common and sometimes justified approach is to say that the observation is *normally* distributed around the true value, i.e.\n",
       "$$\n",
       "\\hat{\\mathbf{x}}_1 = \\mathbf{x}_1 + \\boldsymbol{\\epsilon }\n",
       "$$\n",
       "$$\n",
       "\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0},\\Sigma)\n",
       "$$\n",
       "$$\n",
       "\\Sigma = \\begin{bmatrix} \\sigma^2_{obs} & 0 \\\\ 0 & \\sigma^2_{obs} \\end{bmatrix}\n",
       "$$\n",
       "More concisely, we can write\n",
       "$$\n",
       "P(\\hat{\\mathbf{x}}_1 | \\mathbf{v}_0) = \\mathcal{N}(\\mathbf{x}_1(\\mathbf{v}_0), \\Sigma).\n",
       "$$\n",
       "\n",
       "Finally, we have the denominator term\n",
       "$$\n",
       "P(\\hat{\\mathbf{x}}_1) = \\int P(\\hat{\\mathbf{x}_1}|\\mathbf{v}_0) P (\\mathbf{v}_0) \\; \\mathrm{d}\\mathbf{v}_0,\n",
       "$$\n",
       "which is sometimes called the *evidence*.  This is quite pesky to evaluate because that integral is over all possible values of $\\mathbf{v}_0$.  In fact, for most non-trivial models, it's impossible to compute exactly, and it is from this intractibility that most of the richness of Bayesian statistics emerges.\n",
       "\n",
       "One idea (which is popular in machine learning) is to simply *maximize* $P(\\mathbf{v}_0 | \\hat{\\mathbf{x}}_1)$.  Because $\\hat{\\mathbf{x}}_1$ does not depend upon the velocity, it's a constant and we can ignore it.  However, this does not yield any notion of uncertainty.  Another relatively simple idea is to perform a brute force Monte Carlo integration: randomly sample a large number of $\\mathbf{v}_0$, and compute the integral as \n",
       "$$\n",
       "\\int P(\\hat{\\mathbf{x}_1}|\\mathbf{v}_0) P (\\mathbf{v}_0) \\; \\mathrm{d}\\mathbf{v}_0 \\approx \\frac{1}{N} \\sum_{j=1}^N P(\\hat{\\mathbf{x}_1}|\\mathbf{v}_{0,j}) \n",
       "$$\n",
       "with \n",
       "$\\mathbf{v}_{0,j}$ a sample drawn from a prior distribution over $\\mathbf{v}$.  Of course, we said that this was just going to be proportional to 1, which means that any pair of real numbers is equally likely, and this just won't work.  Instead, let's put a simple bound and say that our initial velocities are between 0 and 20.  \n",
       "\n",
       "It is easiest to see how this works in code.  First, let's make a function that evaluates the posterior up to a multiplicative constant (or rather its logarithm)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b5ce4b",
      "metadata": {},
      "outputs": [],
      "source": [
       "class ProjectilePosterior:\n",
       "    def __init__(self,x_0,g,x_obs,t_obs,sigma2_obs):\n",
       "        self.x_0 = x_0\n",
       "        self.g = g\n",
       "        self.x_obs = x_obs\n",
       "        self.t_obs = t_obs\n",
       "        self.sigma2_obs = sigma2_obs\n",
       "        \n",
       "    def log_posterior(self,v_0):\n",
       "        # This is the forward model\n",
       "        x_pred = self.x_0 + v_0*self.t_obs[:,np.newaxis] + 0.5*self.g*self.t_obs[:,np.newaxis]**2\n",
       "        \n",
       "        # This is the log-likelihood\n",
       "        P = -(0.5*(x_pred - x_obs)**2/sigma2_obs \n",
       "             + 0.5*np.log(sigma2_obs) \n",
       "             + 0.5*(np.log(2*np.pi))).sum()\n",
       "        return P"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "665c6c39",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Observations\n",
       "x_obs = np.array([10,3])\n",
       "t_obs = np.array([1.])\n",
       "\n",
       "# Observational uncertainty\n",
       "sigma2_obs = 0.1\n",
       "\n",
       "# Initial conditions\n",
       "x_0 = np.array([0.,0.])\n",
       "\n",
       "g = np.array([0,-9.81])\n",
       "\n",
       "proj_like = ProjectilePosterior(x_0,g,x_obs,t_obs,sigma2_obs)\n",
       "\n",
       "v_0 = np.array([-10.,1.])\n",
       "print(proj_like.log_posterior(v_0))\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8e7995c0",
      "metadata": {},
      "source": [
       "Let's draw some samples of possible values of $v_0$ and evaluate them."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba68185",
      "metadata": {
       "scrolled": true
      },
      "outputs": [],
      "source": [
       "v_samples = np.random.rand(10000,2)*10 + 5\n",
       "log_posteriors = np.array([proj_like.log_posterior(v) for v in v_samples])\n",
       "posteriors = np.exp(log_posteriors)\n",
       "mean_posteriors = posteriors.mean()\n",
       "posteriors/=mean_posteriors\n",
       "\n",
       "V = 100\n",
       "posteriors/=V\n",
       "\n",
       "plt.scatter(*v_samples.T,c=posteriors)\n",
       "plt.xlabel('vx')\n",
       "plt.ylabel('vy')\n",
       "plt.colorbar()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "1001a08e",
      "metadata": {},
      "source": [
       "We can plot trajectories for each of these samples, with the alpha channel set to be proportional to the probabilities"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "25034890",
      "metadata": {},
      "outputs": [],
      "source": [
       "t = np.linspace(0,1,101)\n",
       "for i in range(10000):\n",
       "    sol = x_0 + np.outer(t,v_samples[i]) + 0.5*np.outer(t**2,g)\n",
       "    plt.plot(sol[:,0],sol[:,1],'k-',alpha=posteriors[i]/posteriors.max())\n",
       "plt.errorbar([x_obs[0]],[x_obs[1]],xerr=3*np.sqrt(sigma2_obs),yerr=3*np.sqrt(sigma2_obs),fmt='ro')\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('y')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "796f2462",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importance sampling - draw from computed samples proportionally to their log posterior\n",
       "random_indices = np.random.choice(len(v_samples),p=posteriors/posteriors.sum(),size=1000)\n",
       "sub_samples = v_samples[random_indices]\n",
       "sub_posteriors = posteriors[random_indices]\n",
       "plt.scatter(*sub_samples.T,c=sub_posteriors)\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "fce75ea9",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.hist(sub_samples[:,0])\n",
       "plt.hist(sub_samples[:,1])"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "32fd150b",
      "metadata": {},
      "source": [
       "This works really well!  Except that we start to run into a problem as the dimensionality of the system increases.  With 10000 points in 2D space, we have around 100 samples per side.  If we were working in 3D space (let's say that we also didn't know one component of the initial position) and had the same number of total random points: now we'd only have $N=10000^{\\frac{2}{3}}$ points for some cross section.  This sampling density would look like this:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbca0a8d",
      "metadata": {},
      "outputs": [],
      "source": [
       "v_samples = np.random.rand(int(10000**(2./3.)),2)*10 + 5\n",
       "log_posteriors = np.array([proj_like.log_posterior(v) for v in v_samples])\n",
       "posteriors = np.exp(log_posteriors)\n",
       "mean_posteriors = posteriors.mean()\n",
       "posteriors/=mean_posteriors\n",
       "\n",
       "plt.scatter(*v_samples.T,c=posteriors)\n",
       "plt.xlabel('vx')\n",
       "plt.ylabel('vy')\n",
       "plt.colorbar()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e025660d",
      "metadata": {},
      "outputs": [],
      "source": [
       "random_indices = np.random.choice(len(v_samples),p=posteriors/posteriors.sum(),size=1000)\n",
       "sub_samples = v_samples[random_indices]\n",
       "sub_posteriors = posteriors[random_indices]\n",
       "plt.scatter(*sub_samples.T,c=sub_posteriors)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "0fcec5ed",
      "metadata": {},
      "source": [
       "This becomes problematic quickly.  An alternative method that doesn't have all the same shortcomings is to use the Metropolis algorithm, which is a type of Markov Chain Monte Carlo (MCMC) method (Monte Carlo because it's random, Markov Chain because we create new samples by making a small perturbation to an existing sample).  The algorithm is quite simple.  We begin with some random value for $\\mathbf{v}_0$.  Next, we make a small modification according to a so-called proposal distribution $$\n",
       "q(\\mathbf{v}_{0,i+1} | \\mathbf{v}_{0,i}) = \\mathcal{N}(\\mathbf{v}_{0,i} | h^2),\n",
       "$$\n",
       "where $h$ is a (usually pretty small) step size.  We then evaluate the posterior for the new location and for the current location.  If\n",
       "$$\n",
       "\\frac{P(\\mathbf{v}_{0,i+1}|\\mathbf{x}_1)}{P(\\mathbf{v}_{0,i|\\mathbf{x}_1})} \\ge 1,\n",
       "$$\n",
       "then we accept the proposed value of $\\mathbf{v}_0$.  If \n",
       "$$\n",
       "\\frac{P(\\mathbf{v}_{0,i+1}|\\mathbf{x}_1)}{P(\\mathbf{v}_{0,i}|\\mathbf{x}_1)} < 1,\n",
       "$$\n",
       "then we accept with probability given by this ratio.  We then repeat this process many times, saving the result of the sampling at each step in the algorithm.  It turns out that the values stored in the chain are samples from the posterior distribution.  Let's see how this works.  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bf2925",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initial guess\n",
       "v_0 = np.array([1.,1.])\n",
       "\n",
       "# Initial unscaled log-probability \n",
       "P_0 = proj_like.log_posterior(v_0)\n",
       "\n",
       "# Initialize the Markov Chain\n",
       "P_chain = [P_0]\n",
       "v_chain = [v_0]\n",
       "\n",
       "# Stepsize\n",
       "h = 0.1\n",
       "\n",
       "# Draw 10000 samples\n",
       "for i in range(10000):\n",
       "\n",
       "    # Propose a new sample\n",
       "    v_prime = v_0 + np.random.randn(2)*h\n",
       "    \n",
       "    # Compute the unscaled log-probability at that location\n",
       "    P_1 = proj_like.log_posterior(v_prime)\n",
       "    \n",
       "    # Compute the logarithm of the ratios\n",
       "    log_ratio = P_1 - P_0\n",
       "    \n",
       "    # Convert back to non-log space\n",
       "    ratio = np.exp(log_ratio)\n",
       "    \n",
       "    # If the new location is more probable than the old one,\n",
       "    # accept the proposal.  If it's not, accept the proposal\n",
       "    # with probability given by the ratio\n",
       "    if ratio>np.random.rand():\n",
       "        v_0 = v_prime\n",
       "        P_0 = P_1\n",
       "        \n",
       "    # Append to the chain (regardless of whether we accepted)\n",
       "    P_chain.append(P_0)\n",
       "    v_chain.append(v_0)\n",
       "\n",
       "# Convert lists to numpy arrays\n",
       "burnin = 0\n",
       "P_chain = np.array(P_chain[burnin:])\n",
       "v_chain = np.array(v_chain[burnin:])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7eb2ad",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.scatter(*v_chain.T,c=np.exp(P_chain))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b831734f",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.hist(v_chain[1000:,0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c4242d",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Compute some sample trajectories from the distribution of initial conditions\n",
       "t = np.linspace(0,1,101)\n",
       "for i in range(1000):\n",
       "    i = np.random.randint(v_chain.shape[0])\n",
       "    sol = x_0 + np.outer(t,v_chain[i]) + 0.5*np.outer(t**2,g)\n",
       "    plt.plot(sol[:,0],sol[:,1],'k-',alpha=0.1)\n",
       "plt.errorbar([x_obs[0]],[x_obs[1]],xerr=3*np.sqrt(sigma2_obs),yerr=3*np.sqrt(sigma2_obs),fmt='ro')\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('y')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "37d1b8fd",
      "metadata": {},
      "source": [
       "As is usually the case, it is a good idea to use an object-oriented model so that we can avoid having to rewrite code.  We can easily write a Metropolis algorithm class that works with a general parameter vector $\\mathbf{m}$."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a864ba",
      "metadata": {},
      "outputs": [],
      "source": [
       "class Metropolis:\n",
       "    def __init__(self):\n",
       "        # Initialize chains\n",
       "        self.P_chain = []\n",
       "        self.m_chain = []\n",
       "        \n",
       "    def sample(self,m_0,log_posterior,h,n_samples,burnin=0,thin_factor=1):\n",
       "        # Compute initial unscaled log-posterior\n",
       "        P_0 = log_posterior(m_0)\n",
       "        \n",
       "        # Add initial location and posterior to the chain\n",
       "        self.P_chain.append(P_0)\n",
       "        self.m_chain.append(m_0)\n",
       "        n = len(m_0)\n",
       "\n",
       "        # Draw samples\n",
       "        for i in range(n_samples):\n",
       "\n",
       "            # Propose new value\n",
       "            m_prime = m_0 + np.random.randn(n)*h\n",
       "\n",
       "            # Compute new unscaled log-posterior\n",
       "            P_1 = log_posterior(m_prime)\n",
       "            \n",
       "            # Compute logarithm of probability ratio\n",
       "            log_ratio = P_1 - P_0\n",
       "            \n",
       "            # Convert to non-log space\n",
       "            ratio = np.exp(log_ratio)\n",
       "            \n",
       "            # If proposed value is more probable than current value, accept.  \n",
       "            # If not, then accept proportional to the probability ratios\n",
       "            if ratio>np.random.rand():\n",
       "                m_0 = m_prime\n",
       "                P_0 = P_1\n",
       "                \n",
       "            # Only append to the chain if we're past burn-in. \n",
       "            if i>burnin:\n",
       "                # Only append every j-th sample to the chain\n",
       "                if i%thin_factor==0:\n",
       "                    self.P_chain.append(P_0)\n",
       "                    self.m_chain.append(m_0)\n",
       "                    \n",
       "        return np.array(self.P_chain),np.array(self.m_chain)\n",
       "\n",
       "# Instantiate sampler\n",
       "sampler = Metropolis()\n",
       "\n",
       "# Draw samples\n",
       "P_chain,m_chain = sampler.sample(v_0,proj_like.log_posterior,0.1,10000,burnin=1000)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "681c84f2",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.scatter(*m_chain.T,c=np.exp(P_chain))"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d983d927",
      "metadata": {},
      "source": [
       "Of course we could do the same thing but with more unknowns.  For example, we could assume that *both* initial velocity and initial position are unknown.  However, before we start, **we should ask whether this problem is well posed.**  "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ac9a90",
      "metadata": {},
      "outputs": [],
      "source": [
       "class ProjectilePosteriorWithX:\n",
       "    def __init__(self,g,x_obs,t_obs,sigma2_obs):\n",
       "        self.g = g\n",
       "        self.x_obs = x_obs\n",
       "        self.t_obs = t_obs\n",
       "        self.sigma2_obs = sigma2_obs\n",
       "        \n",
       "    def log_posterior(self,m_0):\n",
       "        x_0 = m_0[:2]\n",
       "        v_0 = m_0[2:]\n",
       "        x_pred = x_0 + v_0*self.t_obs[:,np.newaxis] + 0.5*self.g*self.t_obs[:,np.newaxis]**2\n",
       "        P = -(0.5*(x_pred - x_obs)**2/sigma2_obs \n",
       "             + 0.5*np.log(sigma2_obs) \n",
       "             + 0.5*(np.log(2*np.pi))).sum()\n",
       "        return P"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10423e6",
      "metadata": {},
      "outputs": [],
      "source": [
       "proj_like = ProjectilePosteriorWithX(g,x_obs,t_obs,sigma2_obs)\n",
       "\n",
       "v_0 = np.array([1.,1.])\n",
       "x_0 = np.array([0,0])\n",
       "\n",
       "# Concatenate intial position *and* velocity into parameter vector\n",
       "m_0 = np.hstack((x_0,v_0))\n",
       "\n",
       "sampler = Metropolis()\n",
       "P_chain,m_chain = sampler.sample(m_0,proj_like.log_posterior,0.1,100000,burnin=1000)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "38509035",
      "metadata": {},
      "outputs": [],
      "source": [
       "t = np.linspace(0,1,101)\n",
       "for i in range(1000):\n",
       "    i = np.random.randint(m_chain.shape[0])\n",
       "    x = m_chain[i,:2]\n",
       "    v = m_chain[i,2:]\n",
       "    sol = x + np.outer(t,v) + 0.5*np.outer(t**2,g)\n",
       "    plt.plot(sol[:,0],sol[:,1],'k-',alpha=0.1)\n",
       "plt.errorbar([x_obs[0]],[x_obs[1]],xerr=3*np.sqrt(sigma2_obs),yerr=3*np.sqrt(sigma2_obs),fmt='ro')\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('y')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "67ccdea1",
      "metadata": {},
      "source": [
       "Lots of possible solutions are consistent with this data!  What if we change up the situation and say that we add a second observation, namely that $\\hat{\\mathbf{x}}_{t=1/2} = \\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} $"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "72eb7789",
      "metadata": {},
      "outputs": [],
      "source": [
       "x_obs = np.array([[5,2],[10,3]])\n",
       "t_obs = np.array([0.5,1.])\n",
       "sigma2_obs = 0.1\n",
       "\n",
       "proj_like = ProjectilePosteriorWithX(g,x_obs,t_obs,sigma2_obs)\n",
       "\n",
       "v_0 = np.array([1.,1.])\n",
       "x_0 = np.array([0,0])\n",
       "m_0 = np.hstack((x_0,v_0))\n",
       "\n",
       "sampler = Metropolis()\n",
       "P_chain,m_chain = sampler.sample(m_0,proj_like.log_posterior,0.1,10000,burnin=1000)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6c0b9c",
      "metadata": {},
      "outputs": [],
      "source": [
       "t = np.linspace(0,1,101)\n",
       "for i in range(1000):\n",
       "    i = np.random.randint(m_chain.shape[0])\n",
       "    x = m_chain[i,:2]\n",
       "    v = m_chain[i,2:]\n",
       "    sol = x + np.outer(t,v) + 0.5*np.outer(t**2,g)\n",
       "    plt.plot(sol[:,0],sol[:,1],'k-',alpha=0.1)\n",
       "plt.errorbar(x_obs[:,0],x_obs[:,1],xerr=3*np.sqrt(sigma2_obs),yerr=3*np.sqrt(sigma2_obs),fmt='ro')\n",
       "plt.xlabel('x')\n",
       "plt.ylabel('y')"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "445d03f3",
      "metadata": {},
      "source": [
       "### MCMC for SIR modelling\n",
       "We can proceed with using the Metropolis algorithm to find the initial conditions *and* parameters of an SIR model in much the same way as above.  The only difference is in the number of parameters (here we'll have 5) and how we compute the proportional log-posterior (it will involve the solution of our ODE!).  \n",
       "\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8d3eee",
      "metadata": {},
      "outputs": [],
      "source": [
       "class SIRPosterior:\n",
       "    def __init__(self,t_obs,u_obs,sigma2_obs):\n",
       "        self.u_obs = u_obs\n",
       "        self.t_obs = t_obs\n",
       "        self.sigma2_obs = sigma2_obs\n",
       "        \n",
       "    def log_posterior(self,log_m):\n",
       "        # We have defined our parameters to sample over as the logarithm\n",
       "        # of the model parameters.  Here we exponentiate them to get\n",
       "        # the representation that we need.  \n",
       "        m = np.exp(log_m)\n",
       "        S_0 = m[0]\n",
       "        I_0 = m[1]\n",
       "        R_0 = m[2]\n",
       "        beta = m[3]\n",
       "        gamma = m[4]\n",
       "        \n",
       "        u0 = np.array([S_0,I_0,R_0])\n",
       "\n",
       "        s = SIR(beta=beta,gamma=gamma)\n",
       "        integrator = om.Integrator(s,method)\n",
       "        t,u = integrator.integrate([times[0],times[-1]],1,u0)\n",
       "        P = -0.5*np.sum((self.u_obs - u)**2)/self.sigma2_obs\n",
       "        return P"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef53b75",
      "metadata": {},
      "outputs": [],
      "source": [
       "S_obs = susceptible/N\n",
       "I_obs = infected/N\n",
       "R_obs = removed/N\n",
       "\n",
       "u_obs = np.c_[S_obs,I_obs,R_obs]\n",
       "\n",
       "sigma2_obs = 0.01\n",
       "\n",
       "sir_like = SIRPosterior(t_obs,u_obs,sigma2_obs)\n",
       "\n",
       "m_0 = np.log(np.array([1.0,0.01,0.1,3.0,1.0]))\n",
       "\n",
       "sampler = Metropolis()\n",
       "P_chain,m_chain = sampler.sample(m_0,sir_like.log_posterior,0.1,10000,burnin=1000)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "139d9f55",
      "metadata": {},
      "outputs": [],
      "source": [
       "for i in range(100):\n",
       "    j = np.random.randint(len(m_chain))\n",
       "    m_best = np.exp(m_chain[j])\n",
       "    S_0 = m_best[0]\n",
       "    I_0 = m_best[1]\n",
       "    R_0 = m_best[2]\n",
       "    beta = m_best[3]\n",
       "    gamma = m_best[4]\n",
       "\n",
       "    u0 = np.array([S_0,I_0,R_0])\n",
       "    s = SIR(beta=beta,gamma=gamma)\n",
       "    integrator = om.Integrator(s,method)\n",
       "    t,u = integrator.integrate([times[0],times[-1]],1,u0)\n",
       "\n",
       "    plt.plot(t,u[:,0],'b-',alpha=0.1)\n",
       "    plt.plot(t,u[:,1],'r-',alpha=0.1)\n",
       "    plt.plot(t,u[:,2],'g-',alpha=0.1)\n",
       "plt.errorbar(times,S_obs,yerr=np.sqrt(sigma2_obs),fmt='bo',label='S_obs',alpha=0.5)\n",
       "plt.errorbar(times,I_obs,yerr=np.sqrt(sigma2_obs),fmt='ro',label='I_obs',alpha=0.5)\n",
       "plt.errorbar(times,R_obs,yerr=np.sqrt(sigma2_obs),fmt='go',label='R_obs',alpha=0.5)\n",
       "plt.legend()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "40c5c0ba",
      "metadata": {},
      "source": [
       "Pretty good fit (although there are details that the model is missing).  One critical piece of information that epidemiologists care about is the basic reproduction number, called $R_0$ which is, roughly speaking, the number of other people that a single infected person is likely to infect.  This number has a direct link to SIR models, because it can be computed as the ratio of $\\beta$ and $\\gamma$.  We can do one better than estimation: we can estimate the distribution of $R_0$ that is consistent with observations.  Let's produce a simple histogram:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdcdf58",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.hist(np.exp(m_chain)[:,3]/np.exp(m_chain)[:,4],density=True)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "333aa65a",
      "metadata": {},
      "source": [
       "It would appear that our estimated $R_0$ is between 2 and 2.7.  This is in strong agreement with the independently estimated COVID-19 values!"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "e29ff05f",
      "metadata": {},
      "source": [
       "### SEIR Model\n",
       "We can make a simple modification to the SIR model that can sometimes help fit diseases for which there is a significant non-infectious incubation period.  Such a model introduces an additional population (called Exposed), and the resulting model is \n",
       "$$\n",
       "\\frac{\\partial S}{\\partial t} = -\\beta S I\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial E}{\\partial t} = \\beta S I - a E\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial I}{\\partial t} = a E - \\gamma I\n",
       "$$\n",
       "$$\n",
       "\\frac{\\partial R}{\\partial t} = \\gamma I.\n",
       "$$\n",
       "**Adapt both the simulation machinery and the model fitting procedure above to work with this SEIR model.  Does such a model yield a better fit to the observations than the SIR model?**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fa6dcc",
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "markdown",
      "id": "e807e1c8",
      "metadata": {},
      "source": [
       "### SEIRS Model\n",
       "Another interesting model that perhaps does a better job of modelling the dynamics of COVID-19 is an SEIRS model, in which those in the recovered population eventually become susceptible again (this should be quite straightforward to include in the model).  **Repeat the process above with an SEIRS model.  Does it improve the fit?  Why or why not?  If you run this model far into the future using parameters based on this relatively short calibration window, what sort of epidemiological conclusions can you make and how certain are you in them?**\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7c6280",
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }